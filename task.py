# -*- coding: utf-8 -*-
import requests
import json
import warnings
import argparse
# Removed matplotlib imports
from tabulate import tabulate
import sys
from collections import OrderedDict
from datetime import datetime, date, timedelta
from dotenv import load_dotenv
import os
import markdown

load_dotenv()

from utils.MailConfig import send_email
# Suppress warnings
from urllib3.exceptions import InsecureRequestWarning
warnings.simplefilter('ignore', InsecureRequestWarning)

# --- Configuration ---
# API_BASE_URL = 'http://platform.localtest.echoing.cc:61002/api/v1'
# ACCESS_TOKEN = 'd16c37694f6b4a65a597d6873181e7cd'
#
login_url = os.getenv("LOGIN_URL")

username = os.getenv("USERNAME")
password = os.getenv("PASSWORD")
login_type = int(os.getenv("LOGIN_TYPE", "2"))

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36',
    'Content-Type': 'application/json',
    'Accept': 'application/json',
}

session = requests.Session()
    "userName": username,
login_data = {
    "password": password,
    "loginType": login_type
}

response = session.post(
    login_url,
    json=login_data,
    headers=headers,
    allow_redirects=True,
    verify=False  # ä»…ç”¨äºæµ‹è¯•ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨çœŸå®è¯ä¹¦
)

if response.status_code == 200:
    try:
        json_response = response.json()
        print("âœ… ç™»å½•å“åº”:", json_response)

        if json_response.get("success") is True:
            data = json_response.get("data", {})
            token = data.get("token") if isinstance(data, dict) else None

            if token:
                ACCESS_TOKEN = token
                print("ğŸ”‘ æˆåŠŸè·å– Token:", ACCESS_TOKEN)

            else:
                print("âŒ æœªæ‰¾åˆ° token å­—æ®µï¼Œè¯·æ£€æŸ¥è¿”å›ç»“æ„")
                sys.exit(1)

        else:
            msg = json_response.get("msg", "æœªçŸ¥é”™è¯¯")
            trace_id = json_response.get("traceId", "æ— è¿½è¸ªID")
            print(f"âŒ ç™»å½•å¤±è´¥: {msg} (Trace ID: {trace_id})")
            sys.exit(1)

    except requests.exceptions.JSONDecodeError:
        print("âš ï¸ å“åº”ä¸æ˜¯æœ‰æ•ˆçš„ JSON")
        print("åŸå§‹å“åº”å†…å®¹:", response.text)
        sys.exit(1)
else:
    print(f"âŒ ç™»å½•å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
    print("å“åº”å†…å®¹:", response.text)
    sys.exit(1)

API_BASE_URL = os.getenv('API_BASE_URL')

TASK_API_URL = f'{API_BASE_URL}/ctrlTaskMng/page'
ENUM_API_URL = f'{API_BASE_URL}/dict/all'

# --- Enums will be fetched dynamically ---
PLATFORMS = None
EVENT_TYPES = None
EXCEPTION_TYPE_NAMES = None

# Default time range (can be overridden if needed)
now = datetime.now()
print('å½“å‰æ—¶é—´:', now)

# è·å–ä»Šå¤©çš„ 00:00:00
today_midnight = now.replace(hour=0, minute=0, second=0, microsecond=0)

# æ˜¨å¤©çš„ 00:00:00
yesterday_midnight = today_midnight - timedelta(days=1)

# è®¾ç½®ä»»åŠ¡æ—¶é—´èŒƒå›´
taskCreateStartTime = yesterday_midnight.strftime("%Y-%m-%d %H:%M:%S")
taskCreateEndTime = today_midnight.strftime("%Y-%m-%d %H:%M:%S")

# Shared Headers
BASE_HEADERS = { 'Accept': 'application/json, text/plain, */*', 'Accept-Language': 'zh_CN', 'Cache-Control': 'no-cache',
                 'Content-Type': 'application/json', 'Origin': 'http://platform.localtest.echoing.cc:61002', 'Pragma': 'no-cache',
                 'Proxy-Connection': 'keep-alive', 'Referer': 'http://platform.localtest.echoing.cc:61002/cloudMachine/loggerList',
                 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36',
                 'accessToken': ACCESS_TOKEN }

STATUS_SUCCESS = 3
STATUS_FAILURE = 4

# Removed CHINESE_FONT_OPTIONS

# --- Feishu Configuration ---
# !!! WARNING: Avoid hardcoding secrets. Use environment variables or config files. !!!
FEISHU_APP_ID = ""  # Replace with your Feishu App ID
FEISHU_APP_SECRET = "" # Replace with your Feishu App Secret
FEISHU_FOLDER_TOKEN = "" # Replace with your target Feishu folder token

# --- End Configuration ---


# --- Enum Fetching Function (Keep as is) ---
def fetch_enums_from_api():
    """Fetches Platform, EventType, and Exception enums from the API."""
    print(f"Attempting to fetch enums from: {ENUM_API_URL}")
    payload = {
        "langKey": "zh_CN",
        "dictKeys": ["ThirdPlatformEnum", "EventTypeEnum", "GroupCtrlClientReportTaskResultEnum"]
    }
    headers = BASE_HEADERS.copy()

    try:
        response = requests.post(ENUM_API_URL, headers=headers, json=payload, verify=False, timeout=15)
        response.raise_for_status()
        data = response.json()

        if not data.get("success") or data.get("code") != 200:
            print(f"[Error] API reported failure while fetching enums: Code={data.get('code')}, Msg={data.get('msg')}")
            return None, None, None

        enum_data = data.get("data", {})

        platforms_list = enum_data.get("ThirdPlatformEnum")
        if platforms_list is None: print("[Error] 'ThirdPlatformEnum' not found."); return None, None, None
        platforms_map = OrderedDict()
        for item in platforms_list:
            code = item.get("code"); desc = item.get("descZh", item.get("desc"))
            if code is not None and desc is not None: platforms_map[code] = desc

        event_types_list = enum_data.get("EventTypeEnum")
        if event_types_list is None: print("[Error] 'EventTypeEnum' not found."); return None, None, None
        event_types_map = {}
        for item in event_types_list:
            code = item.get("code"); desc = item.get("descZh", item.get("desc"))
            if code is not None and desc is not None: event_types_map[code] = desc

        exceptions_list = enum_data.get("GroupCtrlClientReportTaskResultEnum")
        if exceptions_list is None: print("[Error] 'GroupCtrlClientReportTaskResultEnum' not found."); return None, None, None
        exception_types_map = {}
        for item in exceptions_list:
            code = item.get("code"); desc = item.get("desc")
            if code is not None and desc is not None:
                try: exception_types_map[int(code)] = desc
                except (ValueError, TypeError): print(f"[Warning] Skipping invalid code in GroupCtrlClientReportTaskResultEnum: {code}")

        if 0 not in exception_types_map: exception_types_map[0] = "æˆåŠŸ"
        else: exception_types_map[0] = "æˆåŠŸ"; print("[Warning] Overwriting API definition for code 0 with 'æˆåŠŸ'.")

        print("Successfully fetched and processed enums.")
        return platforms_map, event_types_map, exception_types_map

    except requests.exceptions.Timeout: print(f"[Error] Timeout while connecting to enum API: {ENUM_API_URL}"); return None, None, None
    except requests.exceptions.RequestException as e: print(f"[Error] Network error while fetching enums: {e}"); return None, None, None
    except json.JSONDecodeError as e: print(f"[Error] Failed to decode JSON response from enum API: {e}\n    Response text: {response.text[:500]}..."); return None, None, None
    except Exception as e: print(f"[Error] An unexpected error occurred during enum processing: {e}"); return None, None, None


# --- Font Configuration Function (Removed) ---

# --- Feishu API Helper Functions (Keep as is) ---
def get_feishu_tenant_token(app_id, app_secret):
    """è·å–é£ä¹¦ Tenant Access Token"""
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal"
    headers = {"Content-Type": "application/json; charset=utf-8"}
    payload = json.dumps({"app_id": app_id, "app_secret": app_secret})
    try:
        response = requests.post(url, headers=headers, data=payload, timeout=10)
        response.raise_for_status()
        data = response.json()
        if data.get("code") == 0:
            print("é£ä¹¦ Token è·å–æˆåŠŸã€‚")
            return data.get("tenant_access_token")
        else:
            print(f"é£ä¹¦ Token è·å–å¤±è´¥: Code={data.get('code')}, Msg={data.get('msg')}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"è·å–é£ä¹¦ Token æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
        return None
    except json.JSONDecodeError:
        print("è§£æé£ä¹¦ Token å“åº”å¤±è´¥ã€‚")
        return None

def create_feishu_doc(token, folder_token, title):
    """åœ¨æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹åˆ›å»ºæ–°çš„é£ä¹¦æ–‡æ¡£"""
    url = "https://open.feishu.cn/open-apis/docx/v1/documents"
    headers = { "Authorization": f"Bearer {token}", "Content-Type": "application/json; charset=utf-8" }
    payload = json.dumps({"folder_token": folder_token, "title": title})
    try:
        response = requests.post(url, headers=headers, data=payload, timeout=15)
        response.raise_for_status()
        data = response.json()
        if data.get("code") == 0:
            doc_id = data.get("data", {}).get("document", {}).get("document_id")
            print(f"é£ä¹¦æ–‡æ¡£åˆ›å»ºæˆåŠŸ: '{title}' (ID: {doc_id})")
            return doc_id
        else:
            print(f"é£ä¹¦æ–‡æ¡£åˆ›å»ºå¤±è´¥: Code={data.get('code')}, Msg={data.get('msg')}")
            return None
    except requests.exceptions.RequestException as e:
        print(f"åˆ›å»ºé£ä¹¦æ–‡æ¡£æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
        return None
    except json.JSONDecodeError:
        print("è§£æåˆ›å»ºé£ä¹¦æ–‡æ¡£å“åº”å¤±è´¥ã€‚")
        return None

def append_text_block_to_feishu_doc(token, document_id, text_content):
    """å‘é£ä¹¦æ–‡æ¡£è¿½åŠ ä¸€ä¸ªæ–‡æœ¬å—"""
    if not text_content: return False
    url = f"https://open.feishu.cn/open-apis/docx/v1/documents/{document_id}/blocks?document_revision_id=-1"
    headers = { "Authorization": f"Bearer {token}", "Content-Type": "application/json; charset=utf-8" }
    block_data = { "block_type": 2, "text": { "elements": [ {"text_run": {"content": text_content}} ] } }
    payload = json.dumps({"children": [block_data]})
    try:
        response = requests.patch(url, headers=headers, data=payload, timeout=30) # Slightly longer timeout for potentially large content
        response.raise_for_status()
        data = response.json()
        if data.get("code") == 0: return True
        else:
            print(f"è¿½åŠ å†…å®¹åˆ°é£ä¹¦æ–‡æ¡£å¤±è´¥: Code={data.get('code')}, Msg={data.get('msg')}")
            print(f"  Payload snippet: {payload[:200]}..."); return False
    except requests.exceptions.RequestException as e: print(f"è¿½åŠ å†…å®¹åˆ°é£ä¹¦æ–‡æ¡£æ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}"); return False
    except json.JSONDecodeError: print("è§£æè¿½åŠ é£ä¹¦æ–‡æ¡£å“åº”å¤±è´¥ã€‚"); return False

# --- NEW: Dedicated Feishu Sending Function ---
def send_report_to_feishu(report_content, document_title):
    """Handles the entire process of sending the report content to a new Feishu doc."""
    print("åˆå§‹åŒ–é£ä¹¦å‘é€æµç¨‹...")
    if not all([FEISHU_APP_ID, FEISHU_APP_SECRET, FEISHU_FOLDER_TOKEN]):
        print("[é”™è¯¯] é£ä¹¦é…ç½®ä¸å®Œæ•´ (APP_ID, APP_SECRET, FOLDER_TOKEN)ã€‚è¯·åœ¨è„šæœ¬é¡¶éƒ¨è®¾ç½®å®ƒä»¬ã€‚")
        return # Stop Feishu process if config is missing

    feishu_token = get_feishu_tenant_token(FEISHU_APP_ID, FEISHU_APP_SECRET)
    if not feishu_token:
        print("æ— æ³•è·å–é£ä¹¦ Tokenï¼Œå‘é€å¤±è´¥ã€‚")
        return # Stop if token acquisition fails

    print(f"å°è¯•åˆ›å»ºé£ä¹¦æ–‡æ¡£: '{document_title}'")
    new_doc_id = create_feishu_doc(feishu_token, FEISHU_FOLDER_TOKEN, document_title)

    if not new_doc_id:
        print("æœªèƒ½åˆ›å»ºé£ä¹¦æ–‡æ¡£ï¼Œå‘é€å¤±è´¥ã€‚")
        return # Stop if doc creation fails

    print(f"å‡†å¤‡å°†æŠ¥å‘Šå†…å®¹å‘é€åˆ°é£ä¹¦æ–‡æ¡£ ID: {new_doc_id}...")

    # Simple length check (adjust limit as needed based on Feishu API constraints)
    if len(report_content) > 150000: # Example limit
        print("[è­¦å‘Š] æŠ¥å‘Šå†…å®¹å¯èƒ½è¿‡é•¿ï¼Œé£ä¹¦è¿½åŠ å¯èƒ½å¤±è´¥æˆ–è¢«æˆªæ–­ã€‚")

    # Append the content
    success = append_text_block_to_feishu_doc(feishu_token, new_doc_id, report_content)

    if success:
        print(f"æŠ¥å‘Šå·²æˆåŠŸå‘é€åˆ°é£ä¹¦æ–‡æ¡£ '{document_title}' (ID: {new_doc_id})")
    else:
        # Error message printed within append_text_block_to_feishu_doc
        print(f"å‘é€æŠ¥å‘Šå†…å®¹åˆ°é£ä¹¦æ–‡æ¡£ '{document_title}' (ID: {new_doc_id}) å¤±è´¥ã€‚")

# --- End Feishu Sending Function ---


# --- Analysis Functions (Keep as is, but plotting call removed) ---

def get_task_count(task_status, exception_type=None, platform_id=None, event_type=None):
    """æŸ¥è¯¢ä»»åŠ¡è®¡æ•° API"""
    payload = {"pageNum": 1, "pageSize": 1, "taskStatus": task_status,
                "taskCreateStartTime": taskCreateStartTime, "taskCreateEndTime": taskCreateEndTime }
    if platform_id is not None: payload["platform"] = platform_id
    if exception_type is not None: payload["exceptionType"] = exception_type
    if event_type is not None: payload["eventType"] = event_type

    desc_parts = [f"status={task_status}"]
    if platform_id is not None: desc_parts.append(f"platform={platform_id}")
    if exception_type is not None: desc_parts.append(f"exception={exception_type}")
    if event_type is not None: desc_parts.append(f"eventType={event_type}")
    request_desc = ", ".join(desc_parts)

    try:
        response = requests.post( TASK_API_URL, headers=BASE_HEADERS, json=payload, verify=False, timeout=45 )
        response.raise_for_status()
        data = response.json()
        if data.get("success") and data.get("code") == 200:
            total_count = data.get("data", {}).get("totalCount")
            return int(total_count) if total_count is not None else 0
        else: print(f"\nä»»åŠ¡è®¡æ•°APIé”™è¯¯: Code={data.get('code')}, Msg={data.get('msg')} (è¯·æ±‚: {request_desc})"); return None
    except requests.exceptions.Timeout: print(f"\nä»»åŠ¡è®¡æ•°HTTPè¶…æ—¶ (è¯·æ±‚: {request_desc})"); return None
    except requests.exceptions.RequestException as e: print(f"\nä»»åŠ¡è®¡æ•°HTTPè¯·æ±‚é”™è¯¯: {e} (è¯·æ±‚: {request_desc})"); return None
    except (json.JSONDecodeError, ValueError) as e: print(f"\nä»»åŠ¡è®¡æ•°å“åº”è§£æé”™è¯¯: {e} (è¯·æ±‚: {request_desc})"); return None

# --- Plotting Function (Removed) ---
# def plot_failure_distribution(...):

def analyze_task_rates(platform_id=None):
    """è·å–å¹³å°åŸºç¡€æ•°æ®ï¼Œè®¡ç®—æ¯”ç‡ï¼Œè¯†åˆ« Top 3 å¤±è´¥åŸå› """
    platform_name_scope = PLATFORMS.get(platform_id, f"ID {platform_id}") if platform_id is not None else "æ‰€æœ‰å¹³å° (èšåˆ)"
    print(f"\n  æ­£åœ¨åˆ†æèŒƒå›´: {platform_name_scope}...")

    total_success = get_task_count(task_status=STATUS_SUCCESS, platform_id=platform_id)
    if total_success is None: return None
    total_failure = get_task_count(task_status=STATUS_FAILURE, platform_id=platform_id)
    if total_failure is None: return None

    total_tasks = total_success + total_failure
    if total_tasks == 0:
        print(f"    -> æœªæ‰¾åˆ° {platform_name_scope} çš„ä»»åŠ¡ã€‚")
        return { "platform_name": platform_name_scope, "total_tasks": 0, "total_success": 0, "total_failure": 0,
                 "success_rate": 0, "failure_rate": 0, "failure_details": {}, "sum_individual_failures": 0, "top_failures": ["æ— "] * 3 }

    overall_success_rate = (total_success / total_tasks) * 100
    overall_failure_rate = (total_failure / total_tasks) * 100
    failure_details = {}; sum_individual_failures = 0

    if total_failure > 0:
        print(f"    æ­£åœ¨ä¸º {platform_name_scope} è·å–å¤±è´¥è¯¦æƒ… (æŒ‰å¼‚å¸¸ç±»å‹)...")
        exception_codes_to_check = sorted([code for code in EXCEPTION_TYPE_NAMES if code != 0])
        if not exception_codes_to_check: print("      [è­¦å‘Š] æœªæ‰¾åˆ°å¯æ£€æŸ¥çš„å¼‚å¸¸ç±»å‹å®šä¹‰ (é™¤äº†æˆåŠŸ)ã€‚")
        for i, code in enumerate(exception_codes_to_check):
            name = EXCEPTION_TYPE_NAMES.get(code, f"æœªçŸ¥ä»£ç  {code}")
            progress = f"[{i+1}/{len(exception_codes_to_check)}]"
            print(f"\r      {progress} æ£€æŸ¥å¼‚å¸¸ç±»å‹ {code} ({name})...", end='', flush=True)
            count = get_task_count(task_status=STATUS_FAILURE, exception_type=code, platform_id=platform_id)
            if count is None: print(f"\r      {progress} [!] è·å–å¼‚å¸¸ç±»å‹ {code} å¤±è´¥ã€‚è·³è¿‡ã€‚ {' '*20}"); continue
            if count > 0:
                 sum_individual_failures += count; rate_within_failures = (count / total_failure) * 100
                 rate_overall = (count / total_tasks) * 100
                 failure_details[code] = { "name": name, "count": count, "rate_within_failures": rate_within_failures, "rate_overall": rate_overall }
        print(f"\r{' ' * 80}\r      å®Œæˆè·å–å¤±è´¥è¯¦æƒ…ã€‚")

    top_failures_formatted = ["æ— "] * 3
    if failure_details:
        sorted_failures_list = sorted( failure_details.items(), key=lambda item: item[1]['count'], reverse=True )
        for i in range(min(3, len(sorted_failures_list))):
            code, details = sorted_failures_list[i]
            top_failures_formatted[i] = ( f"{details['name']} ({code}): {details['count']} ({details['rate_within_failures']:.1f}%)" )

    print(f"    -> {platform_name_scope} åŸºç¡€åˆ†æå®Œæˆã€‚")
    return { "platform_name": platform_name_scope, "total_tasks": total_tasks, "total_success": total_success,
             "total_failure": total_failure, "success_rate": overall_success_rate, "failure_rate": overall_failure_rate,
             "failure_details": failure_details, "sum_individual_failures": sum_individual_failures, "top_failures": top_failures_formatted }


def analyze_event_type_rates_for_platform(platform_id):
    """è·å–æŒ‡å®šå¹³å°æ¯ä¸ªäº‹ä»¶ç±»å‹çš„æ•°æ®å¹¶è®¡ç®—æˆåŠŸ/å¤±è´¥ç‡"""
    platform_name = PLATFORMS.get(platform_id, f"ID {platform_id}")
    print(f"    æ­£åœ¨ä¸ºå¹³å° '{platform_name}' (ID: {platform_id}) åˆ†æäº‹ä»¶ç±»å‹ç»Ÿè®¡...")
    results_by_event = []
    event_type_codes = sorted(EVENT_TYPES.keys())

    if not event_type_codes: print("      [è­¦å‘Š] æœªæ‰¾åˆ°å¯åˆ†æçš„äº‹ä»¶ç±»å‹å®šä¹‰ã€‚"); return []

    for i, code in enumerate(event_type_codes):
        event_name = EVENT_TYPES.get(code, f"æœªçŸ¥ä»£ç  {code}")
        progress = f"[{i+1}/{len(event_type_codes)}]"
        print(f"\r      {progress} åˆ†æäº‹ä»¶ç±»å‹ {code} ({event_name})...", end='', flush=True)

        success_count = get_task_count(task_status=STATUS_SUCCESS, event_type=code, platform_id=platform_id)
        failure_count = get_task_count(task_status=STATUS_FAILURE, event_type=code, platform_id=platform_id)

        if success_count is None or failure_count is None:
            print(f"\r      {progress} [!] è·å–å¹³å° {platform_id} äº‹ä»¶ç±»å‹ {code} æ•°æ®æ—¶å‡ºé”™ã€‚æ ‡è®°ä¸ºé”™è¯¯ã€‚{' '*10}")
            results_by_event.append([code, event_name, "é”™è¯¯", "é”™è¯¯", "é”™è¯¯", "N/A", "N/A"])
            continue

        total_tasks = success_count + failure_count
        success_rate_str, failure_rate_str = "N/A", "N/A"
        if total_tasks > 0:
            success_rate = (success_count / total_tasks) * 100; failure_rate = (failure_count / total_tasks) * 100
            success_rate_str = f"{success_rate:.2f}%"; failure_rate_str = f"{failure_rate:.2f}%"
        elif total_tasks == 0: success_rate_str = "0.00%"; failure_rate_str = "0.00%"

        results_by_event.append([ code, event_name, total_tasks, success_count, failure_count, success_rate_str, failure_rate_str ])

    print(f"\r{' ' * 80}\r      å®Œæˆå¹³å° '{platform_name}' (ID: {platform_id}) çš„äº‹ä»¶ç±»å‹åˆ†æã€‚")
    return results_by_event

# --- Reporting Functions (Keep as is) ---

def format_detailed_report_markdown(results, platform_name_override=None):
    """å°†å¹³å°åŸºç¡€åˆ†æç»“æœæ ¼å¼åŒ–ä¸º Markdown å­—ç¬¦ä¸²"""
    platform_name = platform_name_override if platform_name_override else results.get("platform_name", "æœªçŸ¥èŒƒå›´")
    report_parts = [f"## è¯¦ç»†ç»“æœ: {platform_name}", f"\n**åˆ†ææ—¥æœŸèŒƒå›´:** {taskCreateStartTime} è‡³ {taskCreateEndTime}"]

    if results is None: report_parts.append("\n```\næœªèƒ½å®Œæˆæ­¤å¹³å°çš„åŸºç¡€åˆ†æã€‚\n```"); return "\n".join(report_parts)

    basic_stats = [f"æŸ¥è¯¢ä»»åŠ¡æ€»æ•°: {results['total_tasks']}", f"  - æˆåŠŸæ€»æ•°: {results['total_success']}", f"  - å¤±è´¥æ€»æ•°: {results['total_failure']}"]
    if results['total_tasks'] > 0: basic_stats.extend(["-" * 30, f"æ€»ä½“æˆåŠŸç‡: {results['success_rate']:.2f}%", f"æ€»ä½“å¤±è´¥ç‡: {results['failure_rate']:.2f}%"])
    else: basic_stats.append("\næœªæ‰¾åˆ°ä»»åŠ¡ï¼Œæ¯”ç‡ä¸é€‚ç”¨ã€‚")
    report_parts.append("\n```\n" + "\n".join(basic_stats) + "\n```")

    report_parts.append("\n### æŒ‰å¼‚å¸¸ç±»å‹åˆ†æå¤±è´¥åŸå› :")
    failure_details = results.get("failure_details", {})
    total_failure = results['total_failure']

    if total_failure > 0 and failure_details:
        headers = ["ä»£ç ", "å¼‚å¸¸åç§°", "æ•°é‡", "å å¤±è´¥æ¯”ä¾‹", "å æ€»ä»»åŠ¡æ¯”ä¾‹"]
        table_data = []
        sorted_failures = sorted(failure_details.items(), key=lambda item: item[1]['count'], reverse=True)
        for code, details in sorted_failures:
            exc_name = EXCEPTION_TYPE_NAMES.get(code, f"æœªçŸ¥ä»£ç  {code}")
            table_data.append([ code, exc_name, details['count'], f"{details.get('rate_within_failures', 0):.2f}%", f"{details.get('rate_overall', 0):.2f}%" ])
        markdown_table = tabulate(table_data, headers=headers, tablefmt="github", stralign="left", numalign="right")
        report_parts.append("\n" + markdown_table)

        sum_individual = results.get('sum_individual_failures', 0)
        if sum_individual != total_failure:
             mismatch = total_failure - sum_individual
             report_parts.append(f"\n> [!è­¦å‘Š] å¼‚å¸¸è®¡æ•°ä¸åŒ¹é…: å„ç±»å‹æ€»å’Œ={sum_individual}, APIæŠ¥å‘Šå¤±è´¥æ€»æ•°={total_failure} (å·®å¼‚={mismatch})")
             if mismatch > 0: report_parts.append(f">  -> å¯èƒ½æœ‰ {mismatch} ä¸ªå¤±è´¥å…·æœ‰æœªçŸ¥/æœªåˆ—å‡ºçš„å¼‚å¸¸ç±»å‹ä»£ç ã€‚")
    elif total_failure > 0: report_parts.append("\nå¤±è´¥æ€»æ•° > 0ï¼Œä½†æœªæ‰¾åˆ°å…·æœ‰å·²çŸ¥å¼‚å¸¸ç±»å‹ä»£ç çš„å…·ä½“å¤±è´¥è®°å½•ã€‚")
    else: report_parts.append("\næ­¤å¹³å°åœ¨æ­¤æœŸé—´æ— å¤±è´¥è®°å½•ã€‚")

    return "\n".join(report_parts)


# def format_event_type_report_markdown(event_type_results, add_main_header=False, header_level=3):
#     """å°†äº‹ä»¶ç±»å‹åˆ†æç»“æœæ ¼å¼åŒ–ä¸º Markdown å­—ç¬¦ä¸²è¡¨æ ¼"""
#     report_parts = []
#     header_prefix = "#" * header_level
#     if add_main_header: report_parts.append(f"{header_prefix} æŒ‰äº‹ä»¶ç±»å‹ç»Ÿè®¡æˆåŠŸ/å¤±è´¥ç‡")
#
#     if not event_type_results: report_parts.append("\næœªèƒ½ç”Ÿæˆäº‹ä»¶ç±»å‹ç»Ÿè®¡æ•°æ®ã€‚"); return "\n".join(report_parts)
#
#     headers = ["äº‹ä»¶ä»£ç ", "äº‹ä»¶æè¿°", "æ€»ä»»åŠ¡æ•°", "æˆåŠŸæ•°", "å¤±è´¥æ•°", "æˆåŠŸç‡", "å¤±è´¥ç‡"]
#     valid_data = [row for row in event_type_results if "é”™è¯¯" not in row]
#     error_data = [row for row in event_type_results if "é”™è¯¯" in row]
#
#     if valid_data:
#         markdown_table = tabulate(valid_data, headers=headers, tablefmt="github", stralign="left", numalign="right")
#         report_parts.append("\n" + markdown_table)
#     else: report_parts.append("\næ— æœ‰æ•ˆçš„äº‹ä»¶ç±»å‹æ•°æ®å¯ä¾›å±•ç¤ºã€‚")
#
#     if error_data:
#         report_parts.append("\n\n**ä»¥ä¸‹äº‹ä»¶ç±»å‹æœªèƒ½æˆåŠŸè·å–æ•°æ®:**")
#         error_headers = ["äº‹ä»¶ä»£ç ", "äº‹ä»¶æè¿°"]
#         error_table_data = [[row[0], row[1]] for row in error_data]
#         error_table = tabulate(error_table_data, headers=error_headers, tablefmt="github")
#         report_parts.append("\n" + error_table)
#
#     return "\n".join(report_parts)

def format_event_type_report_markdown(event_type_results, add_main_header=False, header_level=3):
    """å°†äº‹ä»¶ç±»å‹åˆ†æç»“æœæ ¼å¼åŒ–ä¸º Markdown å­—ç¬¦ä¸²è¡¨æ ¼, å¹¶æŒ‰æŒ‡å®šé¡ºåºæ’åº"""
    report_parts = []
    header_prefix = "#" * header_level
    if add_main_header:
        report_parts.append(f"{header_prefix} æŒ‰äº‹ä»¶ç±»å‹ç»Ÿè®¡æˆåŠŸ/å¤±è´¥ç‡")

    if not event_type_results:
        report_parts.append("\næœªèƒ½ç”Ÿæˆäº‹ä»¶ç±»å‹ç»Ÿè®¡æ•°æ®ã€‚")
        return "\n".join(report_parts)

    headers = ["äº‹ä»¶ä»£ç ", "äº‹ä»¶æè¿°", "æ€»ä»»åŠ¡æ•°", "æˆåŠŸæ•°", "å¤±è´¥æ•°", "æˆåŠŸç‡", "å¤±è´¥ç‡"]
    valid_data = [row for row in event_type_results if "é”™è¯¯" not in row]
    error_data = [row for row in event_type_results if "é”™è¯¯" in row]

    if valid_data:
        # --- Custom Sorting Logic ---
        # Define the desired order for specific event types
        priority_order = ["å‘å¸–", "ä¸»åŠ¨è¯„è®º", "ä¸»åŠ¨ç‚¹èµ", "ä¸»åŠ¨å…³æ³¨"]

        # Create a sort key function
        def get_sort_key(row):
            event_name = row[1] # Event description is at index 1
            if event_name in priority_order:
                # Assign a low number (its index in the priority list) for priority items
                return (priority_order.index(event_name), event_name)
            else:
                # Assign a high number for non-priority items, then sort alphabetically
                return (len(priority_order), event_name)

        # Sort the valid data using the custom key
        sorted_valid_data = sorted(valid_data, key=get_sort_key)
        # --- End Custom Sorting Logic ---

        # Use the sorted data to create the table
        markdown_table = tabulate(sorted_valid_data, headers=headers, tablefmt="github", stralign="left", numalign="right")
        report_parts.append("\n" + markdown_table)
    else:
        report_parts.append("\næ— æœ‰æ•ˆçš„äº‹ä»¶ç±»å‹æ•°æ®å¯ä¾›å±•ç¤ºã€‚")

    if error_data:
        report_parts.append("\n\n**ä»¥ä¸‹äº‹ä»¶ç±»å‹æœªèƒ½æˆåŠŸè·å–æ•°æ®:**")
        error_headers = ["äº‹ä»¶ä»£ç ", "äº‹ä»¶æè¿°"]
        # Sort error data by code for consistency, although order might not matter as much here
        error_table_data = sorted([[row[0], row[1]] for row in error_data], key=lambda x: x[0])
        error_table = tabulate(error_table_data, headers=error_headers, tablefmt="github")
        report_parts.append("\n" + error_table)

    return "\n".join(report_parts)

# --- ä¸»æ‰§è¡Œå— ---
if __name__ == "__main__":

    # Step 1: Fetch Enums
    PLATFORMS, EVENT_TYPES, EXCEPTION_TYPE_NAMES = fetch_enums_from_api()
    if PLATFORMS is None or EVENT_TYPES is None or EXCEPTION_TYPE_NAMES is None:
        print("[è‡´å‘½é”™è¯¯] æ— æ³•ä» API è·å–å¿…è¦çš„æšä¸¾å®šä¹‰ã€‚è„šæœ¬æ— æ³•ç»§ç»­æ‰§è¡Œã€‚")
        sys.exit(1)

    # Step 2: Define Argument Parser
    parser = argparse.ArgumentParser(
        description='åˆ†æä»»åŠ¡æˆåŠŸ/å¤±è´¥ç‡ï¼Œå¯æŒ‰å¹³å°/äº‹ä»¶ç±»å‹ç»Ÿè®¡ï¼Œå¯é€‰è¾“å‡ºåˆ°é£ä¹¦ã€‚æšä¸¾ä» API åŠ¨æ€è·å–ã€‚' # Removed plotting mention
    )
    platform_help_text = f'æŒ‡å®šå¹³å° ID ({", ".join(f"{k}={v}" for k, v in PLATFORMS.items() if k != -1)})ã€‚çœç•¥åˆ™åˆ†ææ‰€æœ‰ã€‚'
    parser.add_argument( '--platform', type=int, required=False, help=platform_help_text )
    # Removed --plot argument
    parser.add_argument( '--feishu', action='store_true', help='å°†æŠ¥å‘Šå‘é€åˆ°é…ç½®çš„é£ä¹¦æ–‡æ¡£æ–‡ä»¶å¤¹ã€‚' )

    # Step 3: Parse Arguments
    args = parser.parse_args()
    platform_to_analyze = args.platform
    # Removed generate_plot_flag
    send_to_feishu_flag = args.feishu

    # Prepare Report Storage
    console_outputs = []
    feishu_markdown_parts = []
    report_title_base = "ä»»åŠ¡åˆ†ææŠ¥å‘Š"
    report_date_str = date.today().strftime("%Y-%m-%d")
    report_title = "" # Will be set based on scope

    # --- Main Logic ---
    if platform_to_analyze is not None:
        # --- Analyze a SINGLE Specific Platform ---
        if platform_to_analyze not in PLATFORMS:
            if platform_to_analyze == -1 and -1 in PLATFORMS: platform_name = PLATFORMS[platform_to_analyze]
            else: print(f"[é”™è¯¯] æ— æ•ˆå¹³å°ID: {platform_to_analyze}ã€‚æœ‰æ•ˆ: {list(k for k in PLATFORMS.keys() if k != -1)}"); sys.exit(1)
        else: platform_name = PLATFORMS[platform_to_analyze]

        report_title = f"{report_title_base} - {platform_name} - {report_date_str}"
        console_outputs.append(f"# {report_title}"); feishu_markdown_parts.append(f"# {report_title}")

        results = analyze_task_rates(platform_id=platform_to_analyze)
        if results:
            platform_report_md = format_detailed_report_markdown(results)
            console_outputs.append(platform_report_md); feishu_markdown_parts.append(platform_report_md)

            event_type_results = analyze_event_type_rates_for_platform(platform_id=platform_to_analyze)
            if event_type_results:
                event_type_table_md = format_event_type_report_markdown(event_type_results, add_main_header=True, header_level=3)
                console_outputs.append("\n" + event_type_table_md); feishu_markdown_parts.append("\n" + event_type_table_md)
            else: msg = f"\n### æœªèƒ½å®Œæˆå¹³å° {platform_name} çš„äº‹ä»¶ç±»å‹åˆ†æã€‚"; console_outputs.append(msg); feishu_markdown_parts.append(msg)

            # Removed plotting call
        else: msg = f"\n## æœªèƒ½å®Œæˆå¹³å° {platform_name} (ID: {platform_to_analyze}) çš„åŸºç¡€åˆ†æã€‚"; console_outputs.append(msg); feishu_markdown_parts.append(msg)

    else:
        # --- Analyze ALL Platforms (Aggregate + Individual) ---
        report_title = f"{report_title_base} - æ‰€æœ‰å¹³å° - {report_date_str}"
        console_outputs.append(f"# {report_title}"); feishu_markdown_parts.append(f"# {report_title}")

        # 1. Aggregate Analysis
        console_outputs.append("## èšåˆåˆ†æ (æ‰€æœ‰å¹³å°)"); feishu_markdown_parts.append("## èšåˆåˆ†æ (æ‰€æœ‰å¹³å°)")
        aggregate_results = analyze_task_rates(platform_id=None)
        if aggregate_results:
            agg_report_md = format_detailed_report_markdown(aggregate_results, platform_name_override="æ‰€æœ‰å¹³å° (èšåˆ)")
            console_outputs.append(agg_report_md); feishu_markdown_parts.append(agg_report_md)
            # Removed plotting call
        else: msg = "\n### æœªèƒ½å®Œæˆæ‰€æœ‰å¹³å°çš„èšåˆåˆ†æã€‚"; console_outputs.append(msg); feishu_markdown_parts.append(msg)

        console_outputs.append("\n---\n\n# å„ç‹¬ç«‹å¹³å°åˆ†æä¸è¯¦æƒ…\n"); feishu_markdown_parts.append("\n---\n\n# å„ç‹¬ç«‹å¹³å°åˆ†æä¸è¯¦æƒ…\n")
        platform_summary_data = []
        summary_headers = ["å¹³å°", "æ€»ä»»åŠ¡æ•°", "æˆåŠŸæ•°", "å¤±è´¥æ•°", "æˆåŠŸç‡", "å¤±è´¥ç‡", "Top 1 å¤±è´¥(å¼‚å¸¸)", "Top 2 å¤±è´¥(å¼‚å¸¸)", "Top 3 å¤±è´¥(å¼‚å¸¸)"]
        platform_ids_to_analyze = [pid for pid in PLATFORMS.keys() if pid != -1]

        # 2. Individual Platform Analysis Loop
        for i, pid in enumerate(platform_ids_to_analyze):
            pname = PLATFORMS.get(pid, f"ID {pid}")
            section_header = f"\n## å¹³å°: {pname} (ID: {pid})\n"
            console_outputs.append("\n" + "="*40 + section_header + "="*40); feishu_markdown_parts.append(section_header)

            individual_results = analyze_task_rates(platform_id=pid)
            if individual_results:
                ind_report_md = format_detailed_report_markdown(individual_results, platform_name_override=pname)
                console_outputs.append(ind_report_md.split('\n', 1)[1]); feishu_markdown_parts.append(ind_report_md.split('\n', 1)[1])

                top_failures = individual_results.get("top_failures", ["æ— "] * 3)
                platform_summary_data.append([ pname, individual_results['total_tasks'], individual_results['total_success'], individual_results['total_failure'], f"{individual_results['success_rate']:.2f}%", f"{individual_results['failure_rate']:.2f}%", top_failures[0], top_failures[1], top_failures[2] ])

                event_type_results = analyze_event_type_rates_for_platform(platform_id=pid)
                if event_type_results:
                    event_type_table_md = format_event_type_report_markdown(event_type_results, add_main_header=True, header_level=3)
                    console_outputs.append("\n" + event_type_table_md); feishu_markdown_parts.append("\n" + event_type_table_md)
                else: msg = f"\n### æœªèƒ½å®Œæˆå¹³å° {pname} çš„äº‹ä»¶ç±»å‹åˆ†æã€‚"; console_outputs.append(msg); feishu_markdown_parts.append(msg)

                # Removed plotting call
            else:
                msg = f"\n### æœªèƒ½å®Œæˆå¹³å° {pname} (ID: {pid}) çš„åŸºç¡€åˆ†æã€‚"; console_outputs.append(msg); feishu_markdown_parts.append(msg)
                platform_summary_data.append([pname, "é”™è¯¯", "é”™è¯¯", "é”™è¯¯", "é”™è¯¯", "é”™è¯¯", "N/A", "N/A", "N/A"])

            if i < len(platform_ids_to_analyze) - 1: hr = "\n---\n"; console_outputs.append(hr); feishu_markdown_parts.append(hr)

        # 3. Summary Table
        console_outputs.append("\n---\n\n## å„ç‹¬ç«‹å¹³å°æ±‡æ€»è¡¨\n"); feishu_markdown_parts.append("\n---\n\n## å„ç‹¬ç«‹å¹³å°æ±‡æ€»è¡¨\n")
        if platform_summary_data:
             summary_title = f"\n**æ±‡æ€»æ—¥æœŸèŒƒå›´:** {taskCreateStartTime} è‡³ {taskCreateEndTime}\n"
             console_outputs.append(summary_title); feishu_markdown_parts.append(summary_title)
             summary_table_md = tabulate(platform_summary_data, headers=summary_headers, tablefmt="github", stralign="left", numalign="right")
             console_outputs.append(summary_table_md); feishu_markdown_parts.append(summary_table_md)
        else: no_summary_msg = "æ— æ³•ç”Ÿæˆå„ç‹¬ç«‹å¹³å°çš„æ±‡æ€»æ•°æ®ã€‚"; console_outputs.append(no_summary_msg); feishu_markdown_parts.append(no_summary_msg)

    # --- Final Output ---
    print("\n\n" + "="*60 + "\n--- å®Œæ•´æŠ¥å‘Šå¼€å§‹ (æ§åˆ¶å°è¾“å‡º) ---" + "\n" + "="*60)
    print('\n'.join(console_outputs))
    print("\n" + "="*60 + "\n--- å®Œæ•´æŠ¥å‘Šç»“æŸ (æ§åˆ¶å°è¾“å‡º) ---" + "\n" + "="*60)

    # åˆå¹¶æ‰€æœ‰éƒ¨åˆ†ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²
    full_markdown_content = "\n".join(feishu_markdown_parts)

    html_content = markdown.markdown(full_markdown_content,extensions=['tables', 'fenced_code'])

    send_email(html_content)

    # å®šä¹‰è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå½“å‰ç›®å½•ä¸‹çš„ output_summary.mdï¼‰
    output_file_path = "README.md"

    # å†™å…¥æ–‡ä»¶
    with open(output_file_path, "w", encoding="utf-8") as f:
        f.write(full_markdown_content)

    print(f"âœ… Markdown æ–‡ä»¶å·²ä¿å­˜è‡³: {output_file_path}")

    # --- Call Feishu Sending Function (Refactored) ---
    if send_to_feishu_flag:
        print("\n" + "="*60 + "\n--- å‘é€åˆ°é£ä¹¦ ---" + "\n" + "="*60)
        final_feishu_content = "\n\n".join(feishu_markdown_parts) # Join parts for Feishu content
        # 'report_title' was set earlier based on analysis scope
        send_report_to_feishu(final_feishu_content, report_title)
        print("="*60 + "\n--- é£ä¹¦å‘é€ç»“æŸ ---" + "\n" + "="*60)

    # --- Script End ---
    print("\nè„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")
